---
title: "Exercise 3"
author: "Kwaku Peprah Adjei, Amir Ahmed"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: preamble.tex
  html_document:
    df_print: paged
    includes:
      in_header: preamble.tex
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( echo = FALSE,
  message=FALSE,
  warning=FALSE,
  include=FALSE)
```

# Problem 1: Markov RF


```{r include=FALSE}
library(ggplot2)
library(geoR)
library(fields)
library(akima)
library(ggpubr)
set.seed(123)
par()
opar <- par()


```


```{r}
seismic <- read.csv("seismic.dat", header = F)
seismic <- as.data.frame(seismic)
names(seismic) <- c("d")
seismic$x <- 0:(nrow(seismic)-1) %/% 75 + 1
seismic$y <- 0:(nrow(seismic)-1)%% 75 + 1
complit <- read.csv("complit.dat", sep = " ")

```

In this exercise, we study problems relating to Mosaic random fields (RF). We have observations of seismic data, denoted by $\lbrace d(\vect x); \vect x \in L \rbrace$ ;$d(\vect x) \in \mathbb{R} $, over a domain $\mathbf{D} \subset \mathbb{R}^2$. The seismic data is collected over a regular $(75 \times 75)$ grid $\mathbf{L}_\mathbf{D}$. 

Moreover, we have observations of the lithology distribution $\{sand, shale\}$ in a geological comparable domain $\mathbf{D}_c \subset \mathbb{R}^2$. These are collected on a regular $(66 \times 66)$ grid $\mathbf{L}_{\mathbf{D}_c}$, which has the same spacing as $\mathbf{L}_\mathbf{D}$, over $\mathbf{D}_c$.

Let the underlying lithology distribution represented by a Mosaic RF be denoted by $\lbrace l(\vect x); \vect x \in L \rbrace ;l(\vect x) \in \{0,1 \}$. We want to identify the underlying lithology distribution over D, the underlying lithology of a point is either sand or shale, $\lbrace 1, 0 \rbrace$ respectively.


## Problem 1a) 

We start by looking at $L_d$.
Let the seismic data collection procedure follow the following likelihood model: 
$$\left[d_i | \vect l \right] = \begin{cases}
0.02 + U_i \text{ if sand, } l_i = 0 \\
0.08 + U_i \text{ if shale, } l_i = 1
\end{cases}$$
$i = 1, 2, \dots, n$. With $U_i$ being identically independently distributed $U_i \sim N(0, 0.06^2)$. This would make each observation point $d_i$ conditionally independent on $\vect l$. That will say: 
\begin{equation}
p(d_i | \vect l) = p(d_i | l_i) = \phi(d_i  ; \mu = 0.02 + 0.06l_i, \sigma^2 = 0.06^2)
\end{equation}	
Where $\phi$ is the pdf of the normal distribution. As all observations are independent we thus have: 
\begin{equation}\label{eq:cond_prob}
 p(\vect d | \vect l) = \prod_{i=1}^{n}p(d_i | l_i) = \prod_{i=1}^{n}  \phi(d_i  ; \mu = 0.02 + 0.06l_i, \sigma^2 = 0.06^2)
\end{equation} 


```{r fig1a1, echo = FALSE, include=TRUE, fig.cap="\\label{fig:1a1} Display of seismic data $L_D$.", fig.height=4, fig.width=4}
# Figure 1

ggplot(seismic, aes(x=x, y=y))+
  geom_raster(aes(fill= d))+
  scale_fill_viridis_c(option="plasma")+
  theme_bw()+
  theme(aspect.ratio = 1)+
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank())
```


We display the observations from $L_D$ as a map in Figure \ref{fig:1a1}, there seems to be one large gathering where $d(\vect x)$ takes on relatively large values, there also seems to be some smaller gatherings of large $d(\vect x)$ in areas centered around the large one.  


\subsection*{Problem 1b)}

```{r fig1b1, echo = FALSE, include=FALSE, fig.cap="\\label{fig:1b1} Display of probabilities", fig.height=4, fig.width=4}

#1b
# Calculating pi-s from data. 
pi <- dnorm(seismic$d, mean = 0.08, sd = 0.06)/(dnorm(seismic$d, mean = 0.02, sd = 0.06) + dnorm(seismic$d, mean = 0.08, sd = 0.06))
new_data <- data.frame(x=seismic$x, y=seismic$y, pi)
ggplot(new_data, aes(x=x, y=y))+
  geom_raster(aes(fill= pi))+
   scale_fill_viridis_c(option="plasma")+
  theme_bw()+
  theme(aspect.ratio = 1)+
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank())

```



We now consider a uniform, independence prior model on $\vect l$. That will say: 
\begin{equation}
p(\vect l) = const
\end{equation}
We note that since the prior is constant we have:
\begin{equation}
p(\vect l | \vect d) \propto p(\vect d | \vect l)
\end{equation}

We get the following posterior model using bayes law and the law of total probability: 
\begin{equation}
p(\vect l | \vect d) = \dfrac{p(\vect d | \vect l)}{\sum_{\vect l \in \mathbb{L}^n}p(\vect d | \vect l)}
\end{equation}
Inserting from \eqref{eq:cond_prob} we get:
\begin{equation}
p(\vect l | \vect d) = \dfrac{ \prod_{i=1}^{n}  \phi(d_i  ; \mu = 0.02 + 0.06l_i, \sigma^2 = 0.06^2)}{\sum_{\vect l' \in \mathbb{L}^n} \prod_{i=1}^{n}  \phi(d_i  ; \mu = 0.02 + 0.06l_i', \sigma^2 = 0.06^2)}
\end{equation}

Where $\mathbb{L}^n$ is the n-dimensional space representing all possible values which $l$ can take. 

As the prior is independent each point would also be conditional independent, for each point we thus get the following. Let: 
\begin{equation}
\begin{split}
p_i &= p(l_i = 1 | d_i) 
\\ &= \dfrac{p(d_i | l_i = 1)}{p(d_i | l_i = 0) + p(d_i | l_i = 1)}
\\ &= \dfrac{\phi(d_i  ; \mu = 0.08, \sigma^2 = 0.06^2)}{\phi(d_i  ; \mu = 0.02, \sigma^2 = 0.06^2) + \phi(d_i  ; \mu = 0.08, \sigma^2 = 0.06^2)}
\end{split}
\end{equation}
As each point either is sand or shale we get:
\begin{equation}
1 - p_i = p(l_i = 0 | d_i)
\end{equation}
We recognize this conditioned model as something Bernoulli-distributed with probability $p_i$. 

We thus have: 
\begin{equation}
E(l_i | d_i) = p_i
\end{equation}
and 
\begin{equation} \label{eq:var}
Var(l_i | d_i) = p_i(1-p_i)
\end{equation}


```{r fig1b2, echo = FALSE, include=TRUE, fig.cap="\\label{fig:1b2} Display of six posterior realizations of $L_D$.", fig.height=8, fig.width=8}


simulate.unif <- function(){
  sapply(pi, function(p) rbinom(n = 1, size = 1, prob = p))
}
gg <- list()
set.seed(1)
for(i in 1:6){
  sim.res <- simulate.unif()
  dat <- data.frame(x=seismic$x,y= seismic$y, z=sim.res)
gg[[i]] <-ggplot(dat, aes(x=y, y=x, fill= factor(z)))+
  geom_raster() +
  theme_bw()+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(aspect.ratio = 1)+
   scale_fill_manual(breaks = levels(factor(dat$z)),
                    values = c("#F7F396", "purple"), labels=c("sand", "shale")) 
}
ggarrange(gg[[1]],gg[[2]], gg[[3]], gg[[4]], gg[[5]], gg[[6]], nrow=2, ncol=3, common.legend = TRUE, legend = "bottom")
```


We simulate 6 trials with the data and display the results in Figure \ref{fig:1b1}. 

The maximum marginal posterior predictor ($MMAP\lbrace \vect l | \vect d \rbrace$) is defined as: 
\begin{equation} \label{eq:mmap}
MMAP\lbrace \vect l | \vect d \rbrace = \vect{\hat l} = argmax_{\vect l \in \mathbb{L}^n}\lbrace p(\vect l | \vect d)\rbrace
\end{equation}

Due to the conditional independence of the points we see: 
\begin{equation}
\hat l_i = \begin{cases}
0, \text{ if } p_i < 0.5 \\
1, \text{ if } p_i \geq 0.5
\end{cases}
\end{equation}
is a MMAP solution. 


```{r fig1b3, echo = FALSE, include=TRUE, fig.cap="\\label{fig:1b3} Plot of MMAP, expected values and variance with uniform prior", fig.height=6, fig.width=6}


# IB MMAP expectance and variance.
ex <- pi 
var <- pi*(1-pi)
MMAP <- pi >= 0.5
MMAP <- as.numeric(MMAP)

dat1 <- data.frame(x=seismic$x, y=seismic$y, z=ex)
ggg1 <- ggplot(dat1, aes(x=x, y=y))+
  geom_raster(aes(fill= z))+
   scale_fill_viridis_c(option="plasma")+
  theme_bw()+
  theme(aspect.ratio = 1)+
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank())+
  ggtitle("E(l|d)")

dat2 <- data.frame(x=seismic$x, y=seismic$y, z=var)
ggg2 <- ggplot(dat2, aes(x=x, y=y))+
  geom_raster(aes(fill= z))+
   scale_fill_viridis_c(option="plasma")+
  theme_bw()+
  theme(aspect.ratio = 1)+
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank())+
  ggtitle("Var(l|d)")
 

dat3 <- data.frame(x=seismic$x,y= seismic$y, z=MMAP)
ggg3 <-ggplot(dat, aes(x=y, y=x, fill= factor(z)))+
  geom_raster() +
  theme_bw()+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(aspect.ratio = 1)+
   scale_fill_manual(breaks = levels(factor(dat$z)),
                    values = c("#F7F396", "purple"), labels=c("sand", "shale"))+
  ggtitle("MMAP")

ggarrange(ggg1, ggg2, ggg3, nrow=2, ncol=2)
```

We plot MMAP solution, expectance and variance in Figure \ref{fig:1b3}. We see that there is relatively high variance is most parts of the map, this is reflected in the large difference we see between the simulations. From both the expected value and the $MMAP$ we see that there is one large spot (top-right) where we expect a large cluster of shale as with some bands of shale on mid-left. This is somewhat reflected in the simulations. A difference betweeen MMAP and the simulations is that the simulations tend to expect more shale than the MMAP. The map of the expected values seems to be closer to the simulations than the MMAP. The MMAP result seems much less noisy. 


## Problem 1c)
Now consider a Markov RF prior model for $\lbrace l(\vect x); \vect x \in L_D \rbrace$. Represented by the n-vector $\vect l$ with the clique system $\vect c_L$ consisting of two closest neighbors on the grid $L_D$. 

The corresponding Gibbs formulation is:
\begin{equation}
	\begin{split}
	p(\vect l) = const \times \prod_{\vect c \in \vect c_L} v_{1l}(l_i, i \in \vect c) &= const \times \prod_{<i, j>\in L_D} \beta^{I(l_i = l_j)} \\
	&= const \times \beta^{\sum_{<i, j>\in L_D} I(l_i = l_j)}
	\end{split}
\end{equation}
With $<i, j> \in L_d$ defining the set of two closest neighbors on the grid $L_D$. 

Want to find expressions for the posterior models and want to specify the Markov formulation for the Markov RF. 

Want to find the Markov formulation for the Markov RF. First see:  
\begin{equation}
	p(l_i | \vect l_{-i}) = \dfrac{p(\vect l)}{\sum_{l_i' \in \mathbb{L}} p(l_i', \vect l_{-i})} = \dfrac{p(\vect l)}{p(l_i = 1, \vect l_{-i}) +p(l_i = 0, \vect l_{-i})} 
 \end{equation}
 This reduces to: 
 \begin{equation}
 	p(l_i | \vect l_{-i}) = p(l_i | l_j, j \in n_i)
 \end{equation}

We note that the joint distribution is given by:
\begin{equation}
	\begin{split}
			p(\vect d, \vect l) &= p(\vect d | \vect l)p(\vect l) 
			\\ &= const \times \prod_{i=1}^{n}p(d_i | l_i) \prod_{\vect c \in \vect c_L} v_{1l}(l_i, i \in \vect c)
			\\ &= const \times \prod_{i=1}^{n}  \phi(d_i  ; \mu = 0.02 + 0.06l_i, \sigma^2 = 0.06^2) \prod_{<i, j>\in L_D} \beta^{I(l_i = l_j)}
	\end{split}
\end{equation}

We input the above into the following:
\begin{equation}
	p(\vect l | \vect d) = \dfrac{p(\vect l, \vect d)}{p(\vect d)} = const  \times \prod_{i=1}^{n}  \phi(d_i  ; \mu = 0.02 + 0.06l_i, \sigma^2 = 0.06^2) \prod_{<i, j>\in L_D} \beta^{I(l_i = l_j)}
\end{equation}
Also: 
\begin{equation}
\begin{split}
	p(l_i, \vect d | \vect l_{-i}) &= p(\vect d | \vect l)p(l_i | \vect l_{-i}) \\ &= p(\vect d | \vect l)p(l_i | \vect l_{-i}) \\ 
	&= 	\dfrac{p(\vect l)}{p(l_i = 1, \vect l_{-i}) +p(l_i = 0, \vect l_{-i})} 	\prod_{i=1}^{n}  \phi(d_i  ; \mu = 0.02 + 0.06l_i, \sigma^2 = 0.06^2) 
\end{split}
\end{equation}
Let $n_i$ be the neighborhood around the ith node. Then have:
\begin{equation}
	p(l_i, \vect d | \vect l_{-i}) = \dfrac{\prod_{l_j \in n_i}\beta^{I(l_i = l_j)}}{\prod_{l_j \in n_i}\beta^{I(0 = l_j)} + \prod_{l_j \in n_i}\beta^{I(1 = l_j)}} 	\prod_{i=1}^{n}  \phi(d_i  ; \mu = 0.02 + 0.06l_i, \sigma^2 = 0.06^2) 
\end{equation} 


Now want to develop expressions for the posterior model  $p(l_i | \vect d, \vect l_{-i})$ have:
\begin{equation}
	\begin{split}
	p(l_i | \vect d, \vect l_{-i}) &= p(l_i | d_i, l_{j} ; j \in n_i) 
	\\ &= \dfrac{p(l_i, d_i, l_j ; j \in n_i)}{p(d_i, l_j ; j \in n_i)}
	\\ &= \dfrac{p(d_i | l_i)\prod_{l_j \in n_i}\beta^{I(l_i = l_j)}}{\phi(d_i|\mu = 0.02, \sigma^2 = 0.06^2)\prod_{l_j \in n_i}\beta^{I(l_j = 0)} + \phi(d_i|\mu = 0.08, \sigma^2 = 0.06^2)\prod_{l_j \in n_i}\beta^{I(l_j = 1)}} 
	\end{split}		
\end{equation} 

as the Markov formulation. 


```{r fig1c1, echo = FALSE, include=TRUE, fig.cap="\\label{fig:1c1}Display of observed data in $D_c$", fig.height=4, fig.width=4}
# c)
plot.map <- function(df, title = "Dc"){
  data <- c()
  for(i in 1:nrow(df)){
    for(j in 1:ncol(df)){
     data <- rbind(data, c(i, j, df[i,j])) 
    }
  }
  data <- as.data.frame(data)
  colnames(data) <- c("x", "y", "l")
  ggplot(data, aes(x=y, y=x, fill= factor(l)))+
  geom_raster() +
  theme_bw()+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(aspect.ratio = 1)+
   scale_fill_manual(breaks = levels(factor(dat$z)),
                    values = c("#F7F396", "purple"), labels=c("sand", "shale"))
}

plot.map(complit)
```



We now display the observations in $D_c$ in Figure \ref{fig:1c1}.

Now want to use these observations to estimate $\beta$ by a maximum pseudo-likelihood procedure. In a optimal solution we would use the assumed $p(\vect l)$ distribution to do this, however we would need $2^n$ calculations to evaluate the normalizing constant, which in our case is infeasible. We rather use the Markov formulation to create an approximation.  (here $\vect d \in \mathbb{L}^n$)

\begin{equation} \label{eq:ising}
	p(\vect d | \beta ) \approx \hat p(\vect d | \beta) = const \times \prod_{i = 1}^{n}\sum_{\lbrace l_i', l_j' | l_j' \in n_i \rbrace \in L }  \prod_{j = i, j \in n_i}p(d_j | l_j')p(l_i'|l_j')
\end{equation}



As we assume the observations to $\vect l$ from $D_c$ be exact the model reduces as:
\begin{equation}
p(d_i | l_i') \rightarrow \delta_{d_i}(l_i') = \begin{cases}
	1, l_i' = l_i \\ 
	0, \text{ else}
	\end{cases}
\end{equation} 

This reduces \eqref{eq:ising} to: 
\begin{equation}
	\hat p(\vect d | \beta) \propto \prod_{i = 1}^n p(d_i|l_j; j \in n_i; \beta) = \prod_{i=1}^{n}\dfrac{\beta^{\sum_{j \in n_i}I(l_i = l_j)}}{\beta^{\sum_{j \in n_i}I(0 = l_j)} + \beta^{\sum_{j \in n_i}I(1 = l_j)}}
\end{equation}

A good estimation of $\beta$ would be to maximize the above giving:
\begin{equation}
	\hat \beta = \text{argmax}_\beta \sum_{i=1}^{n} \left\{ \left( \sum_{j \in n_i}I(l_i = l_j)\right)\log\beta - \log\left(\beta^{\sum_{j \in n_i}I(0 = l_j)} + \beta^{\sum_{j \in n_i}I(1 = l_j)}\right) \right\}
\end{equation}

Using the Optim function in R this value can be "easily" found.  Implementing the function and running we get that $\hat \beta \approx 3.64$.

We can use the information we learned from $D_c$ and apply it to out model on $L_d$ as their lithology is comparable. 

Focus is on realizations from $p(\vect l|\vect d)$, with related predictions $E(\vect l|\vect d)$, variances
in the diagonal terms of $Var(\vect l|\vect d)$, and alternative predictions $MMAP(\vect l|\vect d)$.

To estimate these, we use a MCMC/Gibbs algorithm with a single-site proposal based on the Markov formulation for the Markov RF. The pseudocode for the algorithm is given below. 
\begin{algorithm}[H]
\caption{MCMC/Gibbs algorithm for $p(\vect l| \vect d)$ realizations }
\begin{algorithmic}
	\STATE{$\beta = \hat \beta$}
	\STATE{$\vect l^0 = p(\vect  l^0 | \vect d) > 0$}
	\FORALL{$j = 1,  2, ...$}
		\STATE{$\vect l^j \sim g(\vect l | \vect l^{j-1})$}
	\ENDFOR	
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
	\caption{Function $g(\vect l' | \vect l)$ }
	\begin{algorithmic}
		\STATE{$i \sim Uniform(1, ..., n)$}
		\STATE{$l_i' \sim p(l_i | d_i, l_j, j \in n_i)$} 
		\STATE{$\vect l' = (l_1, ..., l_{i-1}, l_i', l_{i+1}, ..., l_n)$}
		\STATE{return $\vect l'$}
	\end{algorithmic}
\end{algorithm}


```{r}
# Pseudo likelihood 
mmpl <- function(d, beta){
  res <- 0
  for(i in 1:nrow(d)){
    for(j in 1:ncol(d)){
      li <- d[i, j]
      ns <- c()
      if(i != 0){
        ns <- c(ns, d[i-1,j])
      }
      
      if(i != nrow(d)){
        ns <- c(ns, d[i+1,j])
      }
      
      
      if(j != 0){
        ns <- c(ns, d[i,j-1])
      }
      
      if(j != ncol(d)){
        ns <- c(ns, d[i,j+1])
      }
      
      ns <- unlist(ns)
      lj.eq.to.li <- sapply(ns, function(lj) li == lj)
      lj.eq.to.0 <- sapply(ns, function(lj) 0 == lj)
      lj.eq.to.1 <- sapply(ns, function(lj) 1 == lj)
      
      res <- res + sum(lj.eq.to.li)*log(beta) - log(beta^(sum(lj.eq.to.0))+beta^(sum(lj.eq.to.1)))
    }
  }
  res
}

mmpl.vec <- function(d, bs){
  sapply(bs, function(b) mmpl(d = d, beta = b))
}

# Finding best estiamte using optim
opt.res <- optim(fn = function(bs) mmpl.vec(complit, bs),
      par = c(3),
      lower = c(1),
      upper = c(Inf),
      control=list(fnscale=-1),
      method = "L-BFGS-B")


```

```{r}

# Set start beta parameter
beta <- opt.res$par
# Calcukate probability of li being 1
pl.li.1.given.ns <- function(ns, di) {
  lj.eq.to.0 <- sapply(ns, function(lj) 0 == lj)
  lj.eq.to.1 <- sapply(ns, function(lj) 1 == lj)
  
  dnorm(di, mean = 0.08, sd = 0.06) *
    beta^(sum(lj.eq.to.1))/
    (beta^(sum(lj.eq.to.1))*dnorm(di, mean = 0.08, sd = 0.06)+
       beta^(sum(lj.eq.to.0))*dnorm(di, mean = 0.02, sd = 0.06))
}

## Gibbssampler
# Sample from a cell
single.step.cell <- function(i, j, l, d){
  di <- d[i, j]
  li <- l[i, j]
  
  
  ns <- c()
  if(i != 0){
    ns <- c(ns, l[i-1,j])
  }
  
  if(i != nrow(l)){
    ns <- c(ns, l[i+1,j])
  }
  
  
  if(j != 0){
    ns <- c(ns, l[i,j-1])
  }
  
  if(j != ncol(l)){
    ns <- c(ns, l[i,j+1])
  }
  
  # Chance translate to 1 
  p <- pl.li.1.given.ns(ns, di)
  li.next <- rbinom(n = 1, size = 1, prob = p)
  
  l[i, j] <- li.next
  return(l)
}

# Sweep a single time
single.step <- function(l, d){
  for(i in 1:nrow(l)){
    for(j in 1:ncol(l)){
      i <- sample(nrow(l), size = 1)
      j <- sample(ncol(l), size = 1)
      l <- single.step.cell(i, j, l, d)
    }
  }
  
  return(l)
}

d <- matrix(NA, nrow = 75, ncol = 75)

for(i in 1:nrow(seismic)){
  d[seismic[i,]$x,seismic[i,]$y] <- seismic[i,]$d
}

m <- 250
# Sweep m times
gibbs.sampler <- function(l, m, d){
  ls <- c()
  for(q in 1:m){
    print(q)
    l <- single.step(l, d)
    ls <- c(ls, sum(l))
  }
  list(l = l, ls = ls)
}



# # Run m steps with different initial values
# # ALl 1
# l <- matrix(1, nrow = 75, ncol = 75)
# res1 <- gibbs.sampler(l, m, d)
# 
# #Store number of ls in each step, in data frame
# res <- res1$ls
# all.res <- as.data.frame(res)
# all.res$x <- 1:m
# all.res$l <- "All 1"
# # All 0
# l <- matrix(0, nrow = 75, ncol = 75)
# res0 <- gibbs.sampler(l, m, d)
# # Add number of ls in each step, in data frame
# res <- res0$ls
# temp.res <- as.data.frame(res)
# temp.res$x <- 1:m
# temp.res$l <- "All 0"
# all.res <- rbind(all.res, temp.res)
# # Random with 0.5 probabilit
# set.seed(123)
# l <- matrix(rbinom(prob = 0.5, n = 75*75, size = 1), nrow = 75, ncol = 75)
# res.rand <- gibbs.sampler(l, m, d)
# # Add number of ls in each step, in data frame
# res <- res.rand$ls
# temp.res <- as.data.frame(res)
# temp.res$x <- 1:m
# temp.res$l <- "Random matrix"
# all.res <- rbind(all.res, temp.res)
# #Save results
# save(all.res, file = "gibbsinitials.RData")
# Load results, uncomment above if you like to watch paint dry 
load(file = "gibbsinitials.RData")

all.res$l <- as.factor(all.res$l)
all.res$res <- all.res$res/(75*75)
```

```{r fig1c22, echo = FALSE, include=TRUE, fig.cap="\\label{fig:1c22} Display of the proportion of shale using different initial values after 250 iterations.", fig.height=5, fig.width=7}

all.res$`Start value` <- all.res$l
ggplot(all.res) + geom_line(aes(x = x, y = res, color = `Start value`), size = 0.9) + theme_classic() + 
  ylab("Percentage with li = 1") + 
  xlab("Number of steps from initial") + 
  theme(text = element_text(size = 10))

# Starting with random 50-50 seems to have converged fastes, so set burnin to 50, and generate some more samples. This time storing all results
```

```{r}
# Sweep m times and store
m <- 2500
storing.gibbs.sampler <- function(l, m, d){
  ls <- c()
  stored.ls <- list()
  for(q in 1:m){
    print(q)
    l <- single.step(l, d)
    ls <- c(ls, sum(l))
    stored.ls <- c(stored.ls, list(l))
  }
  list(l = l, ls = ls, stored.ls = stored.ls)
}
set.seed(123)
l <- matrix(rbinom(prob = 0.5, n = 75*75, size = 1), nrow = 75, ncol = 75)

# Only run this if you have serious amounts of time
#res.rand <- storing.gibbs.sampler(l, m, d)
#save(res.rand, file = "res_rand.RData")
load("res_rand.RData")
# set burning
burnin <- 50 
```

```{r fig1c2, echo = FALSE, include=TRUE, fig.cap="\\label{fig:c2} Six realisations from the MCMC/Gibbs algorithm.", fig.height=8, fig.width=8}
# Find distance to last
p1 <- plot.map(res.rand$stored.ls[[100]], "Gibbs: Initial iteration")
p2 <- plot.map(res.rand$stored.ls[[500]], "Gibbs: Initial iteration")
p3 <- plot.map(res.rand$stored.ls[[1000]], "Gibbs: Initial iteration")
p4 <- plot.map(res.rand$stored.ls[[1500]], "Gibbs: Initial iteration")
p5 <- plot.map(res.rand$stored.ls[[2000]], "Gibbs: Initial iteration")
p6 <- plot.map(res.rand$stored.ls[[2500]], "Gibbs: Initial iteration")

ggarrange(p1,p2,p3,p4,p5,p6, nrow=2, ncol=3, common.legend = TRUE, legend = "bottom")
```



We implement and run the algorithm for three different initial values. One where we start with $\vect l$ as all zeroes. One where we start with $\vect l$ as all ones. And one where $\vect l$ elements are randomly 1 or 0 with equal chance for each. To check convergence we plot percentage of $l_i$-s that are 1 compared to iteration numbers. The results are displayed in figure \ref{fig:c1}. We run the sampler for 250 iterations in each case. We see that the sampler starting at all 0 and all 1 meet after about 200 iterations we thus assume that they have converged. The random start reach this point the fastest, and seems to keep stationary after about 50 iterations. The one with all ones take 4 times as many iterations to reach the same point. The sampler starting at all 0 seems to slowly move towards the two others, but have not managed to do so after 250 iterations. We conclude that it is best to start with the random matrix. 

To simulate realizations we run 2500 iterations with the sampler starting with the random matrix. We assume burnin after 50 iterations. After that we take each sweep as a sample. First and last iteration is displayed in Figure \ref{fig:c2}. 

```{r}
#Remove burnin:
res <- res.rand$stored.ls[-(1:50)]

# Dont run this, it is slow
# different.df <- c()
# for(i in 1:length(res)){
#   print(i)
#   for(j in 1:length(res)){
#     if(i == j){
#       next()
#     }
# 
#     num.different <- res[[i]] - res[[j]]
#     num.different <- abs(num.different)
#     num.different <- sum(num.different)
#     different.df <- rbind(different.df, c(num.different, abs(i-j)))
#   }
# 
#   if(i %% 10 == 0){
#     write.table(different.df, "myDF.csv", sep = ",", col.names = !file.exists("myDF.csv"), append = T)
#     different.df <- c()
#   }
# 
# }
# 
# df <- read.csv("myDF.csv")
# library(dplyr)
# head(df)
# colnames(df) <- c("index", "num.diff", "dist")
# df <- df %>% group_by(dist) %>% summarise(avg.diff = mean(num.diff))
# save(df, file = "df.RData")
load("df.RData")
```

```{r fig1c3, echo = FALSE, include=TRUE, fig.cap="\\label{fig:c3} Average sample difference, 2450 samples, to check independence", fig.height=7, fig.width=15}
p1 <- ggplot(df) +
  geom_line(aes(x = dist, y = avg.diff)) +
  theme_classic() +
  xlab("Iteration differnce") +
  ylab("Average amount of different cells in map") +
  ggtitle("All iteration differences") + 
  theme(text = element_text(size = 20))

p2 <- ggplot(df) + 
  geom_line(aes(x = dist, y = avg.diff)) + 
  xlim(c(0,100)) +
  theme_classic()+ 
  xlab("Iteration difference") +
  ylab("Average amount of different cells in map") +
  ggtitle("0-100 iteration difference") + 
  theme(text = element_text(size = 20))

p <- ggarrange(p1, p2, ncol = 2, nrow = 1)
annotate_figure(p,
                top = text_grob("Visualizing sample difference after n-iterations", color = "black", face = "bold", size = 27))

```

```{r}
# Keep every 50th  for independence
res <- res[(1:length(res))[1:length(res) %% 50 == 0]]

# number of independent samples:
m <- length(res)
P <- Reduce("+", res)
P <- P/m
P <- as.vector(P) # row then columns

df <- as.data.frame(P)
names(df) <- c("p")
df$x <- 0:(nrow(seismic)-1) %/% 75 + 1
df$y <- 0:(nrow(seismic)-1)%% 75 + 1
df$var <- df$p*(1-df$p)
df$mmap <- df$p >= 0.5 
df$mmap <- as.numeric(df$mmap)
```




To control for sample independence we calculate how many different cells we have between each cell in the sample. We then plot the average difference cointrolled by iteration difference. We remove burnin data. The results are displayed in Figure \ref{fig:c3}. After about 50 iterations the difference seems to be stablizing at approximately 110 different cells in each map. 

We thus decide on taking each 50-th step as a independent sample.

An alternative to this method could i.e. be based on corrolation. 

Using our independent samples we have following estimator:
\begin{equation}
	E(l_i|d_i) = p_i = \dfrac{1}{m}\sum_{j=1}^ml_i^j
\end{equation}


```{r fig1c4, echo = FALSE, include=TRUE, fig.cap="\\label{fig:c4} Estimators from independent gibbs sample", fig.height=6, fig.width=6}
dat4 <- data.frame(x=df$x, y=df$y, z=df$p)
ggg1 <- ggplot(dat4, aes(x=x, y=y))+
  geom_raster(aes(fill= z))+
   scale_fill_viridis_c(option="plasma")+
  theme_bw()+
  theme(aspect.ratio = 1)+
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank())+
  ggtitle("E(l|d)")

dat5 <- data.frame(x=seismic$x,y= seismic$y,z= df$var)
ggg2 <- ggplot(dat5, aes(x=x, y=y))+
  geom_raster(aes(fill= z))+
   scale_fill_viridis_c(option="plasma")+
  theme_bw()+
  theme(aspect.ratio = 1)+
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank())+
  ggtitle("Var(l|d)")
 

dat6 <- data.frame(x=seismic$x, y=seismic$y, z=df$mmap)
ggg3 <-ggplot(dat6, aes(x=y, y=x, fill= factor(z)))+
  geom_raster() +
  theme_bw()+
  theme(legend.title = element_blank(),panel.grid = element_blank(),axis.title = element_blank(),axis.text = element_blank(),axis.ticks = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(aspect.ratio = 1)+
   scale_fill_manual(breaks = levels(factor(dat$z)),
                    values = c("#F7F396", "purple"), labels=c("sand", "shale"))+
  ggtitle("MMAP")

ggarrange(ggg1, ggg2, ggg3, nrow=2, ncol=2)

```


Where $m$ is the number of independent samples, and $l_i^j$ is the $i$-th cell in the $j$-th sample.
Estimators for variancce and MMAP are \eqref{eq:var} and \eqref{eq:mmap} respectively.  The results hare shown in Figure \ref{fig:c4}. 
Compared with the model with the constant prior this model gives a result with a solid body, in the top-right part of the plot. Variance is close to zero, expect at the edges of the solid shale area. The MMAP reflects the expected value, and only shows one connected area where one expect shale. 

## Problem 1d)
Comparing the two methods, the first in 1b) with a uniform prior and the Markov RF in 1c), the Markov RF seems to produce less noise and samples from it seems much more stable and shows much less variance. Samples from the Markov RF seems match what is observed in $D_c$ much better than 1a). Although it takes much more time to implement and run, the Markov RF produces much better results than its competitor.

On the other hand, 1c) might be a bit too strict on what is classified as a shale-area, compared to $D_c$ there seems to be few outlier shale fields, and small shale fields. A possibility to improve on this would be to manually alter the $\beta$ estiamte somewhat.  


In both cases we also note that the MMAP gave satisfying results.


